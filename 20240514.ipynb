{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c27bc-a85f-4952-92d3-0a9a17f4c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20240514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90def28-64d5-478c-a85d-76e2d840446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer 클래스\n",
    "▪ 매개변수 갱신을 위한 기반 클래스\n",
    "▪ 구체적인 최적화 기법은 Optimizer 클래스를 상속한 자식 클래스에서 구현함\n",
    "▪ 초기화 메서드 target과 hooks 라는 두 개의 인스턴스 변수를 초기화\n",
    "▪ setup 메소드는 매개변수를 갖는 클래스를 인스턴스 변수인 target으로 설정\n",
    "▪ update 메서드는 모든 매개변수를 갱신\n",
    "▪ 구체적인 매개변수 갱신은 update_one 메서드에서 수행, 자식 클래스에서 재정의\n",
    "▪ 전처리는 add_hook 메서드를 사용하여 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886848f-05e9-4a88-bf39-59df93f7e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.hooks = []\n",
    "\n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "\n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "\n",
    "    def update_one(self, param):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_hook(self,f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1481b-8436-4207-bdf7-17e0c0405e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD 클래스 구현\n",
    "▪ 경사하강법으로 매개변수를 갱신하는 클래스를 구현\n",
    "▪ SGD 클래스는 Optimizer 클래스를 상속\n",
    "▪ __init__ 메서드는 학습률을 받아 최기화\n",
    "▪ update_one 메서드에서 매개변수 갱신 코드를 구현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf665a31-c098-476a-b2a7-09a28f0ec927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "\n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8f02f-5dd6-451e-8a1c-26c0adbc4391",
   "metadata": {},
   "outputs": [],
   "source": [
    " SGD 클래스를 사용하여 회귀 문제 풀기\n",
    "▪ MLP 클래스를 사용하여 모델을 생성\n",
    "▪ SGD 클래스로 매개변수를 갱신 – optimizer.update( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b842c-3a61-49ab-a330-2a98107c9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "[SGD 이외의 최적화 기법]\n",
    "기울기를 이용한 최적화 기법\n",
    "▪ Momentum, AdaGrad, AdaDelta, Adam\n",
    "▪ Optimizer 클래스를 이용해 다양한 최적화 기법을 필요에 따라 손쉽게 전환\n",
    "▪ Optimizer 클래스를 상속하여 다양한 최적화 기법을 구현\n",
    "\n",
    "Momentum 기법\n",
    "▪ W는 갱신할 가중치 매개변수, 𝜕𝐿/𝜕𝑊은 기울기, η 는 학습률\n",
    "▪ v는 물리에서 말하는 속도에 해당한다.\n",
    "▪ αv 는 물체가 아무런 힘을 받지 않을때 서서히 감속시키는 역할을 한다.\n",
    "    v<-av-η(𝜕𝐿/𝜕𝑊)\n",
    "    W<- W+v 이다.\n",
    "\n",
    " MomentumSGD 구현 코드\n",
    "▪ 속도에 해당하는 데이터, 딕셔너리 타입의 인스턴스 변수 self.vs에 유지\n",
    "▪ 초기화 시에는 vs에 아무것도 담겨있지 않음\n",
    "▪ Update_one() 이 처음 호출될 때 매개변수와 같은 타입의 데이터를 생성\n",
    "▪ 구현한 학습 코드에서 손쉽게 Momentum으로 전환\n",
    "• optimizer = MomentumSGD(lr) 변경하면 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d851410-00b9-4b83-b143-160498e7d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "[소프트맥스 함수와 교차 엔트로피오차]\n",
    " \n",
    "<다중 클래스(Multi-class Classification) 분류>\n",
    "▪ 여러 클래스로 분류하는 문제\n",
    "▪ 분류 대상이 여러 가지 클래스 중 어디에 속하는지 추정함.\n",
    "\n",
    "[슬라이스 조작 함수]\n",
    "\n",
    "<get_item 함수>\n",
    "▪ Variable의 다차원 배열 중에서 일부를 슬라이스하여 뽑아줌\n",
    "▪ (2, 3) 형상의 x에서 1번째 행의 원소를 추출함\n",
    "▪ DeZero 함수로 구현했기 때문에 역전파도 제대로 수행\n",
    "\n",
    "<역전파>\n",
    "▪ y.backward( ) 를 호출하여 역전파 수행\n",
    "▪ 슬라이스로 인한 계산은 다차원 배열의\n",
    "데이터 일부를 수정하지 않고 전달\n",
    "• 원래의 다차원 배열에서 데이터가 추출된\n",
    "위치에 행당 기울기를 설정\n",
    "• 그 외에는 0으로 설정\n",
    "\n",
    "+++코드+++\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "y = F.get_item(x, 1)\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496f759-bb69-4b27-9fd3-98a0cd4d04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[슬라이스 조작 함수]\n",
    "\n",
    "<슬라이스>\n",
    "▪ 다차원 배열의 일부를 추출하는 작업 \n",
    "▪ get_item 함수 사용시 같은 인덱스를 반복 지정하여 동일한 원소를 여러 번 빼낼 수 있음\n",
    "\n",
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "indices = np.array([0,0,1])\n",
    "y = F.get_item(x, indices)\n",
    "print(y)\n",
    "\n",
    "<특수 메서드로 설정>\n",
    "▪ get_item 함수를 Variable 의 메서드로도 사용함. \n",
    "▪ 슬라이스 작업의 역전파도 이루어짐.\n",
    "\n",
    "[소프트맥스 함수]\n",
    "\n",
    "<신경망으로 다중 클래스 분류>\n",
    "▪ 선형 회귀 때 이용한 신경망을 그대로 사용할 수 있음\n",
    "▪ MLP 클래스로 구현해둔 신경망을 그대로 이용할 수 있음\n",
    "▪ 입력 데이터의 차원 수가 2이고 3개의 클래스를 분류하는 문제\n",
    "+++코드+++\n",
    "from dezero.models import MLP\n",
    "model = MLP((10,3))\n",
    "\n",
    "▪ 2층으로 이루어진 완전연결 신경망을 만들어 줌\n",
    "▪ 첫 번째 완전연결 계층의 출력 크기는 10, 두 번째 완전연결계층의 출력 크기는 3\n",
    "▪ model 은 입력 데이터를 3차원 벡터로 변환\n",
    "+++코드+++\n",
    "x = np.array([[0.2, -0.4]])\n",
    "y = model(x)\n",
    "print(y)\n",
    "\n",
    "▪ x의 형상은 (1, 2), 샘플 데이터가 하나 있고 그 데이터는 원소가 2개인 2차원 벡터\n",
    "▪ 신경망의 출력 형태는 (1, 3), 하나의 샘플 데이터가 3개의 원소로 변환\n",
    "▪ (0번, 1번, 2번 원소 중) 2번 원소가 0.92401356 으로 가장 크기 때문에 2번 클래스로 분류\n",
    "\n",
    "<소프트맥스 함수>\n",
    "▪ 신경망 출력은 단순한 수치인데 이 수치를 확률로 변환하기\n",
    "▪ 소프트맥스 함수의 입력 𝑦𝑘 가 총 n개라고 가정할때 K 번째 출력 𝑝𝑘 를 구하는 계산식을 이용.\n",
    "▪ 분자는 입력 𝑦𝑘 의 지수 함수고, 분모는 모든 입력 지수 함수의 총합 ( 0 <= 𝑝𝑖 <= 1).\n",
    "▪ p의 각 원소는 0 이상 1 이하이고, 총합은 1이 됨, 신경망의 출력을 확률로 변환\n",
    "\n",
    "+++코드+++\n",
    "from dezero import Variable, as_variable\n",
    "import dezero.functions as F\n",
    "\n",
    "def softmax1d(x):\n",
    "    x = as_variable(x)\n",
    "    y = F.exp()\n",
    "    sum_y = F.sum(y)\n",
    "    return y / sum_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f066398-a700-482b-bd0a-7b6e7eff6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model - MLP((10,3))\n",
    "x = Variable(np.array([[0.2, -0.4]]))\n",
    "y = model(x)\n",
    "p = softmax1d(y)\n",
    "print(y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e786da25-25a8-4a03-b938-2bcc99a64f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '▪' (U+25AA) (812313216.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ▪ 샘플 데이터 각각에 소프트맥스 함수를 적용하는 경우 배치(batch) 데이터에도 소프트맥수 함수 적용\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '▪' (U+25AA)\n"
     ]
    }
   ],
   "source": [
    "▪ 샘플 데이터 각각에 소프트맥스 함수를 적용하는 경우 배치(batch) 데이터에도 소프트맥수 함수 적용\n",
    "\n",
    "<배치 데이터를 처리하는 소프트맥스 함수>\n",
    "▪ 인수 x는 2차원 데이터로 가정\n",
    "▪ axis = 1 (행축)\n",
    "▪ keepdims=True 이므로 각 행에서 나눗셈\n",
    "▪ 더 나은 구현 방식은 Function 클래스를\n",
    "상속하여 Softmax 클래스를 구현\n",
    "▪ 파이썬 함수로 softmax를 구현하는 것\n",
    "\n",
    "+++코드+++\n",
    "def softmax_simple(x, axis=1)\n",
    "    x = as_variable(x)\n",
    "    y = exp(x)\n",
    "    sum_y = sum(y, axis=axis, keepdims=True)\n",
    "    return y / sum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a9a35-df40-4755-bef8-ef2a25c73701",
   "metadata": {},
   "outputs": [],
   "source": [
    "<교차 엔트로피 오차 (Cross Entropy Error)>\n",
    "             \n",
    "▪ 다중 클래스 분류에 적합한 손실 함수\n",
    "▪ 정답 데이터의 원소는 정답에 해당하는 클래스면 1로, 그렇지 않으면 0으로 기록한다.\n",
    "▪ 이러한 표현 방식을 원핫 벡터 (onehot vector)라 부름.\n",
    "▪ 𝑝𝑘 는 신경망에서 소프트맥스 함수를 적용한 후의 출력한다.\n",
    "▪ 정답 클래스에 해당하는 번호의 확률 p를 추출함으로써 교차 엔트로피의 오차를 계산한다.\n",
    "▪ P[t]는 벡터 p에서 t번째 요소만을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b35f0-3005-409b-990b-9313e649df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "<교차 엔트로피 오차 구현>\n",
    "\n",
    "▪ 인수 x는 신경망에서 소프트맥스 적용하기 전의 출력, t는 정답 데이터\n",
    "▪ clip 함수는 x_min 이하면 x_min 으로 변환하고, x_max 이상이면 x_max로 변환\n",
    "▪ np_arrange(N)은 [0, 1, …, N-1] 형태의 ndarray 인스턴스를 생성해 줌\n",
    "\n",
    "+++코드+++\n",
    "def softmax_cross_entropy_simple(x, t):\n",
    "    x, t = as_variable(x), as_variable(t)\n",
    "    N = x.shape(0)\n",
    "    p = softmax(x)\n",
    "    p = clip(p, 1e-15, 1.0)\n",
    "    log_p = log(p)\n",
    "    tlog_p = log_p[np.arange(N). t.data]\n",
    "    y = -1 * sum(tlog_p) / N\n",
    "    return y\n",
    "\n",
    "▪ x와 정답 데이터 t를 가지고 교차 엔트로피 오차를 계산\n",
    "\n",
    "+++코드+++\n",
    "x = np.array([[0.2, -0.4],[0.3, 0.5],[1.3, -3.2],[2.1, 0.3]])\n",
    "t = np.array([2, 0, 1, 0])\n",
    "y = model(x)\n",
    "loss = F.softmax_cross_entropy_simple(y,t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05f6d2-1508-4f41-a3e6-cbf48e2cfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "[다중 클래스 분류]\n",
    "\n",
    "<다중 클래스 분류 수행>\n",
    "▪ 소프트맥스 함수와 교차 엔트로피 오차를 구현한다.\n",
    "▪ 스파이럴 데이터셋 이라는 작은 데이터셋을 사용하여 다중 클래스 분류 실제 수행한다.\n",
    "(스파이럴은 나선형 혹은 소용돌이 모양이라는 뜻 )\n",
    "\n",
    "<스파이럴 데이터셋>\n",
    "▪ get_spiral 함수를 사용하여 스파이럴 데이터셋을 읽어오기.\n",
    "\n",
    "+++코드+++\n",
    "import dezero.datasets as ds\n",
    "\n",
    "x, t = ds.get_spiral(train = True)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])\n",
    "\n",
    "▪ train=True 면 학습용 데이터를 반환, False 면 테스트용 데이터를 반환\n",
    "▪ 반환되는 값은 입력 데이터인 x와 정답 데이터인 t 임\n",
    "▪ x와 t는 모두 ndarray 인스턴스이며, 형상은 각각 (300, 2) 와 (300, ) 임\n",
    "▪ 문제는 3클래스 분류이므로 t이 원소는 0, 1, 2 중 하나가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2d904-567c-41db-a5de-2d3ae22ab32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "<학습 코드>\n",
    "▪ (1) 하이퍼파라미터 설정 – 은닉층 수와 학습률\n",
    "▪ (2) 데이터 셋을 읽고 모델과 옵티마이저를 생성\n",
    "        -max_epoch = 300, 미니배치로 배치 크기는 30으로 설정\n",
    "▪ (3) np.random.permutation 함수를 사용하여 데이터 셋의 인덱스를 무작위로 섞음\n",
    "            (무작위로 정렬된 색인 리스트를 새로 생성)\n",
    "▪ (4) 미니배치 생성\n",
    "            ( 미니배치의 인덱스는 방금 생성한Index에서 앞에서부터 차례로 꺼내 사용)\n",
    "▪ (5) 기울기를 구하고 매개변수 갱신\n",
    "▪ (6) 에포크마다 손실 함수 결과 출력\n",
    "\n",
    "+++코드+++\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "#(1)하이퍼파라미터설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "#(2) 데이터 읽기, 모델과 옵티마이저 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "#(3)데이터 셋을 읽고 모델과 옵티마이저를 생성\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        #(4)미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        #(5)기울기를 구하고 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "        #(6)에포크마다 손실 함수 결과 출력\n",
    "        avg_loss = sum_loss / dta_size\n",
    "        print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n",
    "\n",
    "+++이 이후부터는 코드 생략 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd53f1-0d7b-4a6d-a78d-566a68aee268",
   "metadata": {},
   "outputs": [],
   "source": [
    "<코드 실행 후 손실 그래프 생성>\n",
    "▪ 학습을 진행할수록 손실이 줄어들어듬을 확인.\n",
    "▪ 학습이 완료된 신경망의 클래스 영역에 대한 결정 경계 시각화\n",
    "\n",
    "<대규모 데이터셋의 필요성>\n",
    "▪ 스파이럴 데이터셋은 작은 데이터셋이라서 ndarray 인스턴스 하나로 처리\n",
    "▪ 데이터가 100만 개라면?\n",
    "• 거대한 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올려야만 함\n",
    "▪ 대규모 데이터를 처리할 수 있도록 데이터셋 전용 클래스인 Dataset 클래스를 만듬\n",
    "▪ Dataset 클래스에는 데이터를 전처리할 수 있는 구조도 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45244d1c-52a4-4007-bcf8-791b30cee464",
   "metadata": {},
   "outputs": [],
   "source": [
    "<Dataset 클래스 구현>\n",
    "▪ Dataset 클래스는 기반 클래스로서의 역할을 함\n",
    "▪ 실제로 사용하는 데이터셋은 이를 상속하여 구현\n",
    "▪ prepare 메서드가 데이터 준지 작업을 하도록 구현\n",
    "▪ __getitem__ 메서드는 단순히 지정된 인덱스에 위치하는 데이터를 꺼냄\n",
    "▪ __len__ 메서드는 데이터셋의 길이를 알려줌\n",
    "\n",
    "<큰데이터셋의 경우>\n",
    "▪ 작은 데이터셋은 Dataset 클래스의 인스턴스 변수인 data와 label에 직접 ndarray 인스턴스\n",
    "를 유지해도 무리가 없음\n",
    "▪ 큰 데이터셋의 구현 방식은 지금의 방식을 사용할 수 없음\n",
    "▪ data 디렉터리와 label 디렉터리에 각가 100만 개의 데이터가 저장되어 있다고 가정\n",
    "▪ 빅 데이터 처리 방법\n",
    "• BigData 클래스를 초기화할 떄는 데이터를 읽지 않음\n",
    "• 데이터에 접근하는 __getitem__(index)가 불리는 시점에 데이터를 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e3d71-e050-4a1e-9406-b69cff793efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "<신경망 입력 형태로의 데이터 준비>\n",
    "▪ 신경망을 학습시킬 때는 데이터 셋 중 일부를 미니배치로 꺼냄\n",
    "▪ 인덱스를 지정하여 batch에 여러 데이터가 리스트로 저장하고, ndarray 인스턴스로 변환\n",
    "        (데이터를 DeZero 의 신경망에 입력하기 위해)\n",
    "▪ batch 의 각 원소에서 데이터만을 꺼내 하나의 ndarray 인스턴스로 변형함(이어 붙임) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731040a-346f-4649-bd36-8f6abb6183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "<Spiral 클래스를 사용하여 학습시 달라진 점>\n",
    "    \n",
    "▪ Spiral 클래스 사용시 미니배치를 만드는 부분의 코드를 수정\n",
    "▪ Dataset 클래스를 사용하여 신경망을 학습\n",
    "- 이점은 다른 데이터셋으로 교체해 학습할 때 확인할 수 있음\n",
    "- Spiral을 BigData 로 교체하는 것만으로 훨씬 큰 데이터셋을 대응 할 수 있음\n",
    "▪ 데이터셋 인터페이스를 통일하여 다양한 데이터셋을 똑같은 코드로 처리 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023b02a-cece-4035-9acc-44f874debd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<데이터셋 전처리>\n",
    "    \n",
    "▪ 모델에 데이터를 입력하기 전에 데이터를 특정한 형태로 가공할 경우가 많음\n",
    "▪ 데이터 형상 변형, 데이터 확장 등등\n",
    "▪ 초기화 시에 transform 과 target_transform 을 새롭게 받음\n",
    "▪ transform 은 입력 데이터 하나에 대한 변환을 처리하고, target_transform 레이블 하나에\n",
    "대한 변환을 처리함\n",
    "( 이 값이 None 이면 전처리 로직은 lambda x : x 로 설정되어 받은 인수를 그대로 변환)\n",
    "▪ 데이터를 스케일 변환 한다면 데이터셋에 사용자가 원하는 전처리를 추가할 수 있다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
