{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c5fe96-45c6-45f8-a1cd-67eb9ffd83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20240507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345073ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<선형회귀를 구현>\n",
    "토이 데이터셋\n",
    "▪ 실험용으로 만든 작은 데이터셋\n",
    "\n",
    "예측 모델\n",
    "▪ x와 y라는 두 개의 변수로 구성된 데이터 셋을 생성하며 둘은 선형관계\n",
    "▪ x값이 주어지면 y값을 예측하는 모델을 만드는 것이며 y에 추가된 노이즈 때문에 점들이 구름같은 양상을 띈다.\n",
    "\n",
    "코드:\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = 5+2*x+np.random.rand(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eec034",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예측 모델의 목표\n",
    "▪ 주어진 데이터를 잘 표현하는 함수 찾기\n",
    "▪ y와 x가 선형 관계라고 가정하여, y = Wx + b 식으로 표현할 수 있음\n",
    "▪ 데이터와 예측치의 차이는 잔차(residual)인데, 이를 최소화해야한다. \n",
    "\n",
    "평균 제곱 오차 (Mean Squared Error)\n",
    "▪ 예측치(모델)와 데이터의 오차를 나타내는 지표\n",
    "▪ 표현되는 손실 함수의 출력을 최소화하는 W와 b를 찾는 함수의 최적화 문제이다.\n",
    "▪ 경사하강법을 사용하여 최소화하는 매개변수를 찾는다.\n",
    "\n",
    "데이터의 예측치를 구하는 predict 함수 구현\n",
    "▪ 매개변수 W와 b를 Variable 인스턴스로 생성\n",
    "▪ matmul 함수를 사용하여 행렬의 곱을 계산\n",
    "▪ X와 W의 대응하는 차원의 원소 수가 일치(D차원인 경우 W의 형상이 (D, 1)), y 형상은 (100, 1) 임\n",
    "코드:\n",
    "'''\n",
    "W = Variable(np.zeros(1,1))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "y = F.matmul(x, W) +b\n",
    "return y\n",
    "\n",
    "'''\n",
    "▪ x.shape[1] 과 W.shape[0]을\n",
    "일치시켜야 행렬 곱이 제대로 계산됨\n",
    "▪ 100개의 데이터 각각에 대해 W에 의한\n",
    "벡터의 내적을 계산\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4051781",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "평균 제곱 오차를 구하는 mean_squared_error 함수 구현\n",
    "코드:\n",
    "def mean_sqaured_error(x0, x1):\n",
    "diff = x0 -x1\n",
    "return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "경사하강법으로 매개변수 갱신\n",
    "▪ 매개변수를 갱신할 때 W.data -= lr * W.grad.data 처럼 인스턴스 변수의 data에 대한 계산\n",
    "▪ 매개변수 갱신은 단순히 데이터를 갱신함\n",
    "▪ 코드를 실행하면, 손실함수의 출력값이\n",
    "줄어드는 것을 확인할 수 있음\n",
    "▪ W = Variable([2.11807369]), b = Variable([5.46608905]) 라는 값을 얻음\n",
    "\n",
    "\n",
    "<최종코드>\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "#토이데이터 설정\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = 5+2*x+np.random.rand(100,1)\n",
    "x, y =Variable(x), Variable(y)\n",
    "\n",
    "W = Variable(np.zeros((1,1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "y = F.matmul(x, W) +b\n",
    "return y\n",
    "\n",
    "def mean_sqaured_error(x0, x1):\n",
    "diff = x0 -x1\n",
    "return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "y_pred = predict(x)\n",
    "loss = mean_squared_errpor(y, y_pred)\n",
    "\n",
    "W.cleargrad()\n",
    "b.cleargrad()\n",
    "loss.backward()\n",
    "\n",
    "W.data -= lr * W.grad.data\n",
    "b.data -= lr * b.grad.data\n",
    "print(W, b, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mean_squared_error 함수 구현의 문제점\n",
    "▪ 중간에 등장하는 이름 없는 변수 세 개는 계산 그래프가 존재하는 동안 메모리에 계속 살아있음. \n",
    "▪ 이 변수들의 데이터 (ndarray 인스턴스)도 마찬가지로 계속 살아 있음\n",
    "▪ 컴퓨터의 메모리가 충분하다면 지금의 구현 방식도 문제 없음\n",
    "▪ 메모리 최적화를 위해 더 나은 방식을 도입해야함\n",
    "\n",
    "-> 함수 개선.\n",
    "\n",
    "mean_squared_error 함수 개선\n",
    "▪ Function 클래스를 상속하여 구현하는 방식\n",
    "▪ 순전파 , ndarray 인스턴스, 역전파를 구현\n",
    "• mean_squared_error 함수에서 구현한 코드와 거의 동일\n",
    "• 수식으로 미분을 계산한 다음 해당 수식을 코드로 구현\n",
    "▪ 중간에 등장하던 변수들이 사라짐\n",
    "(forward 메서드에서만 사용됨)\n",
    "▪ 새로 구현한 mean_squared_error 함수는 앞서 구현한 함수와 같은 결과를 얻지만 메모리는 덜 사용함\n",
    "\n",
    "코드:\n",
    "'''\n",
    "class MeanSquaredError(Function):\n",
    "def forward(self, x0, x1):\n",
    "diff = x0 - x1\n",
    "y = (diff ** 2).sum() / len(diff)\n",
    "return y\n",
    "\n",
    "def backward(self, gy):\n",
    "x0, x1 = self.inputs\n",
    "diff = x0 - x1\n",
    "gx0 = gy * diff * (2. / len(diff))\n",
    "gx1 = -gx0\n",
    "return gx0, gx1\n",
    "\n",
    "def mean_sqaured_error(x0, x1):\n",
    "return MeanSquaredError()(x0,x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<선형 회귀 구현을 신경망으로 확장>\n",
    "\n",
    "선형 변환 혹은 아핀 변환(Affine Transformation) \n",
    "▪ 입력 x와 매개변수 W 사이에서 행렬 곱을 구하고 거기에 b를 더함\n",
    "▪ 선형 변환은 신경망에서는 완전연결계층에 해당.\n",
    "선형 변환을 linear 함수로 구현하는 방식\n",
    "▪ Function 클래스를 상속하여 새롭게 Linear라는 함수 클래스를 구현.\n",
    "▪ Linear 함수 클래스 구현 방식이 메모리를 더 효율적으로 사용함.\n",
    "▪ DeZero의 matmul 함수와 +(add 함수)를 이용한 방식: matmul 함수의 출력은 Variable 인스턴스이므로 계산 그래프에 기록됨\n",
    "▪ Function 클래스를 상속하여 Linear 클래스를 구현하는 방식: 중간 결과가 Variable 인스턴스로 보존되지 않기 때문에 순전파 시 사용하던 중간 데이터는 순전파가 끝나는 즉시 삭제됨.\n",
    "\n",
    "DeZero 의 함수를 사용하면서 메모리 효율 개선하는 방법\n",
    "▪ 변수 t는 matmul 함수의 출력인 동시에 +(add) 함수의 입력. +의 역전파는 출력 쪽의 기울기를 단순히 흘려보낸다.\n",
    "▪ matmul 역전파는 입력 x, W, b 만 사용, t의 데이터는 + 역전파와 matmul 역전파에 필요로 하지 않음\n",
    "▪ 기울기를 흘려보내야 하므로 계산 그래프에서는 변수 t가 필요하지만, 그 안의 데이터는 즉시 지워도 상관X.\n",
    "\n",
    "\n",
    "linear_simple 함수 구현\n",
    "▪ 인수 x와 W는 Variable 인스턴스 혹은 ndarray 인스턴스라고 가정\n",
    "▪ ndarray 인스턴스라면 matmul 함수 안에서 Variable 인스턴스로 변환\n",
    "▪ B = None 인 경우는 단순히 행렬 곱셈만 계산하여 결과를 반환\n",
    "▪ 편향이 주어지면 더해준다.\n",
    "▪ t.data = None 코드에서 t 데이터를 메모리에서 삭제.\n",
    "\n",
    "코드:\n",
    "'''\n",
    "def linear_simple(x,W,b=None):\n",
    "t = matmul(x,W)\n",
    "if b is None:\n",
    "return t\n",
    "\n",
    "y= t + b\n",
    "t.data = None #t데이터를 메모리에서 삭제\n",
    "return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " 비선형 데이터셋\n",
    "▪ 선형 회귀로는 문제를 풀수 없는 데이터셋이지만 신경망을 이용하여 해결할 수 있다.\n",
    "\n",
    " 비선형 데이터 셋 생성\n",
    "▪ sin 함수를 사용하여 데이터를 생성\n",
    "▪ 생성한 (x, y) 점들을 2차원 평면에 그려보면 ^v의 형태.\n",
    "\n",
    "코드: \n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35321ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "활성화 함수\n",
    "▪ 선형 변환은 입력 데이터를 선형으로 변환해 줌\n",
    "▪ 신경망은 선형 변환의 출력에 비선형 변환을 수행한다. 그리고 비선형 변환이 활성화 함수이다. ( ReLU, 시그모이드 함수 등이 있음)\n",
    "\n",
    "시그모이드 함수\n",
    "▪ 활성화 함수로 시그모이드 함수를 사용하여 신경망을 구현하고 이 비선형 변환이 텐서의 각 원소에 적용된다.\n",
    "\n",
    "시그모이드 함수 코드:\n",
    "'''\n",
    "def sigmoid_simple(x):\n",
    "x = as_variable(x)\n",
    "y = 1 / (1 + exp(-x))\n",
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e15bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "신경망 구현 : 신경망 추론 코드\n",
    "▪ 일반적인 신경망 형태는 연속적으로 변환을 수행한다.\n",
    "(선형 변환 -> 활성화 함수 -> 선형 변환 -> 활성화 함수 ->….)\n",
    "▪ 선형 변환과 활성화 함수를 순서대로 적용\n",
    "▪ 추론의 정확성을 높이려면 학습이 필요함\n",
    "▪ 신경망 학습에서는 추론을 처리한 후 손실 함수를 추가한다.\n",
    "▪ 손실 함수의 출력을 최소화하는 매개변수를 찾아야하며 선형 변환이나 활성화 함수 등에 의한 변환을 층(Layer)라고 한다.\n",
    "\n",
    "\n",
    "실제 데이터셋을 활용하여 신경망 학습\n",
    "(1) 매개변수 초기화\n",
    "• I는 입력층의 차원 수, H는 은닉층의 차원 수, O는 출력층의 차원 수, 편향은 0 벡터로 초기화\n",
    "(2) 신경망 추론을 수행\n",
    "(3) 매개변수 갱신\n",
    "▪ 비선형 관계도 제대로 학습함\n",
    "▪ 이 구현 방식을 적용하여 더 깊은 신경망도 구현할 수 있다.\n",
    "\n",
    "코드:\n",
    "'''\n",
    "import numpy as np\n",
    "from dezero import Variable\n",
    "import dezero.functions as F\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2 * np.pi * x)+np.random.rand(100,1)\n",
    "\n",
    "#가중치 초기화\n",
    "I, H, O = 1, 10, 1\n",
    "W1 = Variable(0.01 * np.random.randn(I,H))\n",
    "b1 = Variable(np.zeros(H))\n",
    "W2 = Variable(0.01 * np.random.randn(H,O))\n",
    "b2 = Variable(np.zeros(O))\n",
    "\n",
    "#신경망 추론\n",
    "\n",
    "def predict(x):\n",
    "y = F.linear(x, W1, b1)\n",
    "y = F.sigmoid(y)\n",
    "y = F.linear(y, W2, b2)\n",
    "return y\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "#신경망 학습\n",
    "for i in range(iters):\n",
    "y_pred = predict(x)\n",
    "loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "W1.cleargrad()\n",
    "b1.cleargrad()\n",
    "W2.cleargrad()\n",
    "b2.cleargrad()\n",
    "loss.backward()\n",
    "\n",
    "W1.data -= lr * W1.grad.data\n",
    "b1.data -= lr * b1.grad.data\n",
    "W2.data -= lr * W2.grad.data\n",
    "b2.data -= lr * b2.grad.data\n",
    "if i % 1000 == 0:\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc95454",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DeZero 신경망 프레임워크의 사용 편의성 개선\n",
    "▪ 지금의 DeZero는 사용 편의성 면에서 몇 가지 문제가 있음\n",
    "▪ 층이 깊어 질수록 매개변수 관리가 번거로워 짐\n",
    "( 매개변수의 기울기를 재설정하거나 매개변수를 갱신하는 등의 작업)\n",
    "▪ 매개변수 관리를 간소화하는 구조 필요\n",
    "( 더욱 복잡한 신경망 구현시 매개변수를 다루는 일도 그 만큼 복잡해 짐) \n",
    "->매개변수를 담는 구조를 만듬 : Parameter와 Layer 라는 클래스를 구현하여 매개변수 관리를 자동화할 수 있음\n",
    "\n",
    "Parameter 클래스\n",
    "▪ Parameter 클래스는 Variable 클래스와 똑같은 기능을 갖게 함\n",
    "▪ Variable 클래스를 상속한 것 뿐이라 기능도 Variable 클래스와 동일함\n",
    "\n",
    "코드:\n",
    "class Parameter(Variable):\n",
    "pass\n",
    "\n",
    "Parameter 인스턴스와 Variable 인스턴스 구별\n",
    "▪ Parameter 인스턴스와 Variable 인스턴스를 조합하여 계산할 수 있으며 isinstance 함수로 구분할 수 있음\n",
    "▪ 이 점을 이용하여 Parameter 인스턴스만을 담는 구조를 만들 수 있다.\n",
    "\n",
    "코드:\n",
    "import numpy as np\n",
    "from dezero import Variable, Parameter\n",
    "\n",
    "x = Variable(no.array(1.0))\n",
    "y = Parameter(np.array(2.0))\n",
    "y = x * p\n",
    "\n",
    "print(isinstance(p, Parameter))\n",
    "print(isinstance(x, Parameter))\n",
    "print(isinstance(y, Parameter))\n",
    "\n",
    "결과:\n",
    "TRUE\n",
    "FALSE\n",
    "FALSE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ff4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Layer 클래스\n",
    "▪ Function 클래스와 마찬가지로 변수를 변환하는 클래스\n",
    "▪ 매개변수를 유지한다는 점이 다르다. Layer 클래스를 기반 클래스로 두고 구체적인 변환은 자식 클래스에서 구현함\n",
    "Layer 클래스 구현\n",
    "1)\n",
    "▪ _params 라는 인스턴스 변수는 Layer 인스턴스에 속한 매개변수를 보관.\n",
    "▪ __setattr__ 메서드는 인스턴스 변수를 설정할 때 호출.\n",
    "▪ 이 메서드를 override 하면 인스턴스 변수를 설정할 때 커스텀 로직을 추가할 수 있음.\n",
    "\n",
    "코드:\n",
    "from dezero.core import Parameter\n",
    "\n",
    "class Layer:\n",
    "def __init__(self):\n",
    "self._params = set()\n",
    "\n",
    "def __setattr__(self, name, value):\n",
    "if isinsrance(value, Parameter):\n",
    "self._params.add(name)\n",
    "super().__setattr__(name, value)\n",
    "\n",
    "2) 매개변수를 인스턴스 변수 _params 저장\n",
    "▪ value가 Parameter 인스턴스라면 self._params에 name을 추가함\n",
    "▪ name을 추가해 Layer 클래스가 갖는 매개변수를 인스턴수 변수 _params에 모아둘 수 있음\n",
    "\n",
    "layer = Layer()\n",
    "\n",
    "layer.p1 = Parameter(np.array(1))\n",
    "layer.p2 = Parameter(np.array(2))\n",
    "layer.p3 = Parameter(np.array(3))\n",
    "layer.p4 = 'test'\n",
    "\n",
    "print(layer._params)\n",
    "print('--------------')\n",
    "\n",
    "for name in layer._params:\n",
    "print(name, layer.__dict__[name])\n",
    "\n",
    "\n",
    "결과:\n",
    "{'p2', 'p1'}\n",
    "--------\n",
    "p2 variable(2)\n",
    "p1 variable(1)\n",
    "\n",
    "\n",
    "\n",
    "▪ layer 인스턴스 변수를 설정하면 Parameter 인스턴스를 보유하고 있는 변수 이름만 layer._params에 추가됨\n",
    "▪ 인스턴스 변수 __dict__에는 모든 인스턴스 변수가 딕셔너리 타입으로 저장되므로 Parameter 인스턴스만 꺼낼 수 있음\n",
    "\n",
    "(3)Layer 클래스의 4개의 메소드 추가\n",
    "__call__ 메서드\n",
    "• 입력받은 인수를 건네 forward 메서드 호출\n",
    "• 출력이 하나뿐이라면 튜플이 아니라 그 출력을 직접 반환\n",
    "• 입력과 출력 변수를 약한 참조로 유지\n",
    "\n",
    "params 메서드\n",
    "• Layer 인스턴스에 담겨 있는 Parameter인스턴스들을 꺼내줌\n",
    "\n",
    "cleargrads 메서드\n",
    "• 모든 매개변수의 기울기를 재설정함\n",
    "\n",
    "yield 반환\n",
    "• 처리를 일시 중지하고 값을 반환\n",
    "• yield 를 사용하면 작업을 재개할 수 있음\n",
    "\n",
    "코드(class Layer 안에 구현):\n",
    "'''\n",
    "def __call__(self, *inputs):\n",
    "outputs = self.forward(*inputs)\n",
    "if not isinstance(outputs, tuple):\n",
    "outputs = (outputs,)\n",
    "self.inputs = [weakref.ref(x) for x in inputs]\n",
    "self.outputs = [weakref.ref(y) for y in outputs]\n",
    "return outputs if len(outputs) > 1 else outputs[0]\n",
    "\n",
    "def forward(self, inputs):\n",
    "raise NotImplementedError()\n",
    "\n",
    "def params(self):\n",
    "for name in self._params:\n",
    "yield self.__dict__[name]\n",
    "\n",
    "def cleargrads(self):\n",
    "for params in self.params():\n",
    "params.cleargrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "선형 변환을 하는 Linear 클래스 구현\n",
    "▪ 계층으로서의 Linear 클래스를 구현\n",
    "▪ Linear 클래스는 Layer 클래스를 상속하여 구현\n",
    "▪ 가중치와 편향은 두 Parameter 인스턴스 변수의 이름이 self._params에 추가됨\n",
    "▪ forward 메서드로 선형 변환을 구현 (linear 함수를 호출)\n",
    "\n",
    "코드:\n",
    "'''\n",
    "class Linear(Layer):\n",
    "def __init__(self, in_size, out_size, nobias=False, dtype=np.float32):\n",
    "super().__init__()\n",
    "\n",
    "I, O = in_size, out_size\n",
    "W_data = np.random.randn(I, O).astype(dtype) * np.sqrt(1 / I)\n",
    "self.W = Parameter (W_data, name ='W')\n",
    "if nobias:\n",
    "self.b = None\n",
    "else:\n",
    "self.b = Parameter(np.zeros(0, dtype=dtype), name='b')\n",
    "\n",
    "def forward(self,x):\n",
    " y = F.linear(x, self.W, self.b)\n",
    "return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear 클래스 구현하는 더 나은 방법\n",
    "▪ 가중치 W를 생성하는 시점을 늦추는 방식: 가중치를 초기화 메서드가 아닌 forward 메서드에서 생성함으로써 Linear 클래스의 입력 크기를 자동으로 결정할 수 있음\n",
    "▪ __init__ 메서드에서 in_size를 지정하지 않아도 됨\n",
    "▪ forward(self, x) 메서드에서 입력 x의 크기에 맞게 가중치 데이터를 생성\n",
    "▪ layer = Linear(100) 처럼 출력 크기만 지정해도 됨\n",
    "\n",
    "Linear 클래스를 이용하여 신경망을 구현\n",
    "▪ 이전 문제 sin 함수의 데이터셋에 대한 회귀 문제를 다시 풀어봄\n",
    "▪ 주목할 점은 매개변수 관리를 Linear 인스턴스가 맡고 있음\n",
    "▪ 매개변수 기울기 재설정과 매개변수 갱신 작업이 전보다 깔끔해짐\n",
    "▪ Linear 클래스를 개별적으로 다루는 부분이 추후 깊은 신경망 구현을 위해 개선되어야 함.\n",
    "\n",
    "(기존 코드는 건너뛰고 확장 버전의 코드를 다루겠음)\n",
    "\n",
    "확장된 새로운 Layer 클래스 구조\n",
    "▪ Layer 클래스는 매개변수를 관리하는 구조라 이를 사용하면 매개변수를 사용자가 직접 다루지 않아도 되어 편리하지만 마찬가지로 Layer 인스턴스 자체도 관리가 필요하다.(10층 신경망을 구현하려면 10개의 Layer 인스턴스를 관리해야 함)\n",
    "▪ 확장된 Layer 클래스는 여러 개의 Parameter를 가질 수 있다.\n",
    "▪ Layer 클래스 안에 다른 Layer 가 들어가는 구조이며 바깥 Layer 에서 그 안에 존재하는 모든 매개변수를 꺼낼 수 있도록 한다.\n",
    "▪ 첫 번째 변화: 인스턴스 변수를 설정할 때 Layer 인스턴스의 이름도 _params에 추가하도\n",
    "록 함\n",
    "▪ 두 번째 변경점: 매개변수를 꺼내는 처리를 name에 해당하는 객체 obj 가 Layer 인스턴스라면 obj.params() 을 호출, Layer 속 Layer에서도 매개변수를 재귀적으로 꺼낼 수 있도록 한다.\n",
    "\n",
    "새로운 Layer 클래스를 사용하여 신경망 구현\n",
    "▪ model = Layer( )에서 인스턴스 생성한 다음 model 인스턴스 변수로 Linear 인스턴스 추가\n",
    "▪ model.params( )로 model 내에 존재하는 모든 매개변수에 접근할 수 있음\n",
    "▪ model.cleargrads( )는 모든 매개변수의 기울기를 재설정함\n",
    "\n",
    "코드:\n",
    "'''\n",
    "class Layer:\n",
    "def __init__(self):\n",
    "self._params = set()\n",
    "\n",
    "def __setattr__(self, name, value):\n",
    "if isinstance(value, (Parameter, Layer)):\n",
    "self._params.add(name)\n",
    "super().__setattr__(name, value)\n",
    "\n",
    "def params(self):\n",
    "for name in self._params:\n",
    "obj = self.__dict__[name]\n",
    "\n",
    "if isinstance(obj, Layer):\n",
    "yield from obj.params()\n",
    "else:\n",
    "yield obj\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "from dezero import Layer\n",
    "\n",
    "model = Layer()\n",
    "model = L.Linear(5)\n",
    "model = L.Linear(3)\n",
    "\n",
    "def predict(model, x):\n",
    "y = model.l1(x)\n",
    "y = F.sigmoid(y)\n",
    "y = model.12(y)\n",
    "return y\n",
    "\n",
    "#모든 매개변수에 접근\n",
    "for p in model.params():\n",
    "print(p)\n",
    "\n",
    "#모든 매개변수의 기울기를 재설정\n",
    "model.cleargrads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Layer 클래스를 더 편리하게 사용하는 방법\n",
    "▪ Layer 클래스를 상속하여 모델 전체를 하나의 클래스로 정의하는 방법\n",
    "▪ TwoLayerNet 이름으로 클래스 모델을 정의하낟. 이 클래스는 Layer를 상속함\n",
    "▪ __init__ 메서드에서는 필요한 Layer들을 생성하여 self.l1.. 형태로 설정\n",
    "▪ forward 메서드에는 추론을 수행하는 코드를 작성\n",
    "▪ TwoLayerNet 클래스 하나에 신경망에 필요한 모든 코드를 집약할 수 있음\n",
    "\n",
    "코드:\n",
    "'''\n",
    "class TwoLayerNet(Model):\n",
    "def __init__(self, hidden_size, out_size):\n",
    "super().__init__()\n",
    "self.l1 = L.Linear(hidden_size)\n",
    "self.l2 = L.Linear(out_size)\n",
    "\n",
    "def forward(self,x):\n",
    "y = F.sigmoid(self.l1(x))\n",
    "y = self,l2(y)\n",
    "return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a76957",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "모델을 표현하기 위한 Model 클래스 생성\n",
    "▪ 신경망도 수식으로 표현할 수 있는 함수이며 그것을 가르켜 모델이라고 함\n",
    "▪ Model 클래스는 Layer 클래스의 상속\n",
    "▪ 시각화를 위한 plot 메서드는 인수 *inputs로 전달받은 데이터를 forward 메서드로 계산 후\n",
    "생성된 계산 그래프를 이미지 파일로 내보냄\n",
    "▪ Model 클래스는 마치 Layer 클래스처럼 활용할 수 있음\n",
    "\n",
    "코드:\n",
    "'''\n",
    "from dezero import Variable, Model\n",
    "import dezero.layers as L\n",
    "import dezero.functions as F\n",
    "\n",
    "class TwoLayerNet(Model):\n",
    "def __init__(self, hidden_size, out_size):\n",
    "super().__init__()\n",
    "self.l1 = L.Linear(hidden_size)\n",
    "self.l2 = L.Linear(out_size)\n",
    "\n",
    "def forward(self, x):\n",
    "y = F.sigmoid(self.l1(x))\n",
    "y = self.l2(y)\n",
    "return y\n",
    "\n",
    "x = Variable(np.random.randn(5, 10), name('x'))\n",
    "model = TwoLayerNet(100, 10)\n",
    "model.plot(x)\n",
    "\n",
    "#model 클래스를 사용한 문제해결\n",
    "model = TwoLayerNet(hidden_size, 1)\n",
    "\n",
    "#학습 시작\n",
    "for i in range(max_iter):\n",
    "y_pred = model(x)\n",
    "loss = F.mean_squared_error(y,y_pred)\n",
    "\n",
    "model.cleargrad()\n",
    "loss.backward()\n",
    "\n",
    "for p in model.params():\n",
    "p.data -= lr * p.grad.data\n",
    "if i % 1000 == 0:\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이외의클래스: MLP -완전연결계층 신경망 구현\n",
    "▪ fc_output_sizes는 신경망을 구성하는 완전연결 계층들의 출력 크기를 튜플 또는 리스트로 지정함\n",
    "((10, 1)을 건네면 2개의 Linear 계층을 만들고\n",
    "첫 번째 계층의 출력 크기는 10, 두번째 계층 출력 크기는 1로 구성)\n",
    "▪ 계층 모델에 인스턴스 변수를 설정하는 식으로 계층에 포함된 매개변수들을 관리함\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
